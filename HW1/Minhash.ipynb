{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34f25bf",
   "metadata": {},
   "source": [
    "# MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5228422",
   "metadata": {},
   "source": [
    "## In this question you will implement a MinHash class to generate a signature to calculate jaccard similarity between sets.\n",
    "### [Part 1] Please follow the following steps\n",
    "1. Fill in generate_signature function using the hash_functions list\n",
    "2. Implement jaccard_similarity function that takes two minhash signatures as input and returns their estimated similarity\n",
    "3. Implement jaccard_original function, which calculates the exact jaccard similarity by using the original elements of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af66232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.seed(0)\n",
    "class MinHash:\n",
    "    def __init__(self, num_hashes):\n",
    "        self.num_hashes = num_hashes\n",
    "        self.hash_functions = self._generate_hash_functions()\n",
    "\n",
    "    def _generate_hash_functions(self):\n",
    "        def _hash(x):\n",
    "            return hash(str(x))\n",
    "        \n",
    "        hash_functions = []\n",
    "        for i in range(self.num_hashes):\n",
    "            next_prime = self._next_prime(i)\n",
    "            hash_functions.append(lambda x, i=i: (_hash(x) + i) % next_prime)\n",
    "        return hash_functions\n",
    "\n",
    "    def _next_prime(self, n):\n",
    "        def is_prime(num):\n",
    "            if num < 2:\n",
    "                return False\n",
    "            for i in range(2, int(num ** 0.5) + 1):\n",
    "                if num % i == 0:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        prime = n\n",
    "        while not is_prime(prime):\n",
    "            prime += 1\n",
    "        return prime\n",
    "    \n",
    "    def generate_signature(self, lst):\n",
    "        #This function generates a minhash signature for the elements in lst\n",
    "        #Approach: \n",
    "            #Think of hash function as returning the rank of elements in the permutation\n",
    "            #Element of lst that has the minimum hash is the first element in the permutation that belongs to lst\n",
    "        signature = []\n",
    "        for hash_function in self.hash_functions:\n",
    "            min_hash = float('inf')\n",
    "            for element in lst:\n",
    "                hash_value = hash_function(element)\n",
    "                if hash_value < min_hash:\n",
    "                    min_hash = hash_value\n",
    "            signature.append(min_hash)\n",
    "        return signature\n",
    "\n",
    "def jaccard_similarity(minhash1, minhash2):\n",
    "\n",
    "    assert len(minhash1) == len(minhash2)\n",
    "    num_agree = 0\n",
    "    for i in range(len(minhash1)):\n",
    "        if minhash1[i] == minhash2[i]:\n",
    "            num_agree += 1\n",
    "    return num_agree / len(minhash1)\n",
    "\n",
    "\n",
    "def jaccard_original(lst1,lst2):\n",
    "    set1 = set(lst1)\n",
    "    set2 = set(lst2)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54eaef2",
   "metadata": {},
   "source": [
    "# [Part 2] Test Minhash on the following datasets and choose num_hashes\n",
    "\n",
    "Pick num_hashes such that the estimated jaccard similarity is within 0.10 of the original similarity. You can just vary num_hashes to pick the smallest value which ensures difference less than 0.10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07e7f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with num_hashes =  2650\n",
      "Trying with num_hashes =  2675\n",
      "Trying with num_hashes =  2688\n",
      "Trying with num_hashes =  2694\n",
      "Trying with num_hashes =  2697\n",
      "Trying with num_hashes =  2696\n",
      "Trying with num_hashes =  2695\n",
      "Below are the estimated and original similarities\n",
      "i, j, estimated sim, accurate sim\n",
      "0 1 0.3068645640074211 0.25586995785671285 Difference is less than 0.1\n",
      "0 2 0.9335807050092765 0.9242081447963801 Difference is less than 0.1\n",
      "0 3 0.022634508348794064 0.010676156583629894 Difference is less than 0.1\n",
      "0 4 0.7272727272727273 0.6806470940683044 Difference is less than 0.1\n",
      "1 2 0.288682745825603 0.24426605504587157 Difference is less than 0.1\n",
      "1 3 0.033024118738404454 0.0 Difference is less than 0.1\n",
      "1 4 0.15064935064935064 0.05783456624075319 Difference is less than 0.1\n",
      "2 3 0.013358070500927645 0.0 Difference is less than 0.1\n",
      "2 4 0.700556586270872 0.6565366972477065 Difference is less than 0.1\n",
      "3 4 0.012244897959183673 0.0 Difference is less than 0.1\n"
     ]
    }
   ],
   "source": [
    "#Please give the path of downloaded files here\n",
    "path = '/Users/zq84/CS6386/HW1/datarepo/'\n",
    "files=['5kru-rjmf.csv','23z9-6uk9.csv','29bv-qqsy.csv','5g59-fxev.csv','29ry-u5bf.csv']\n",
    "\n",
    "low, high = 2600, 2700\n",
    "while low < high:\n",
    "    mid = (low + high) // 2\n",
    "    print(\"Trying with num_hashes = \", mid)\n",
    "    minhash = MinHash(mid)\n",
    "    hash_lst = []\n",
    "    val_lst = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(path + file)\n",
    "        if 'DBN' in df.columns:\n",
    "            hashval = minhash.generate_signature(list(df['DBN'].values))\n",
    "            val_lst.append(list(df['DBN'].values))\n",
    "        else:\n",
    "            hashval = minhash.generate_signature(list(df['dbn'].values))\n",
    "            val_lst.append(list(df['dbn'].values))\n",
    "        hash_lst.append(hashval)\n",
    "\n",
    "    all_within_tolerance = True\n",
    "    for i in range(len(hash_lst)):\n",
    "        for j in range(i + 1, len(hash_lst)):\n",
    "            min_hash_jaccard = jaccard_similarity(hash_lst[i], hash_lst[j])\n",
    "            original_jaccard = jaccard_original(val_lst[i], val_lst[j])\n",
    "            if abs(min_hash_jaccard - original_jaccard) > 0.1:\n",
    "                all_within_tolerance = False\n",
    "                break\n",
    "        if not all_within_tolerance:\n",
    "            break\n",
    "\n",
    "    if all_within_tolerance:\n",
    "        high = mid\n",
    "    else:\n",
    "        low = mid + 1\n",
    "\n",
    "num_hashes = low\n",
    "minhash = MinHash(num_hashes)\n",
    "\n",
    "\n",
    "#Calculates jaccard similarity\n",
    "hash_lst=[]\n",
    "val_lst=[]\n",
    "for file in files:\n",
    "    df=pd.read_csv(path+file)\n",
    "    if 'DBN' in df.columns:\n",
    "        hashval=minhash.generate_signature(list(df['DBN'].values))\n",
    "        val_lst.append(list(df['DBN'].values))\n",
    "    else:\n",
    "        hashval=minhash.generate_signature(list(df['dbn'].values))\n",
    "        val_lst.append(list(df['dbn'].values))\n",
    "    hash_lst.append(hashval)\n",
    "\n",
    "print (\"Below are the estimated and original similarities\")\n",
    "print (\"i, j, estimated sim, accurate sim\")\n",
    "i=0\n",
    "while i<len(hash_lst):\n",
    "    j=i+1\n",
    "    while j<len(hash_lst):\n",
    "        min_hash_jaccard=jaccard_similarity(hash_lst[i], hash_lst[j])\n",
    "        original_jaccard=jaccard_original(val_lst[i], val_lst[j])\n",
    "        if abs(min_hash_jaccard-original_jaccard)>0.1:\n",
    "            print (i,j,min_hash_jaccard,original_jaccard, \"Difference is more than 0.1\")\n",
    "        else:\n",
    "            print (i,j,min_hash_jaccard,original_jaccard, \"Difference is less than 0.1\")\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93bdcc",
   "metadata": {},
   "source": [
    "# [Part 3] We will now use Minhash to implement LSH\n",
    "Given: The function has already initialized 1 bucket for each band (in __init__ function)\n",
    "1. Implement index function to add each dataset to a bucket for each band\n",
    "    You need to do the following\n",
    "    a. Split signature into different bands\n",
    "    b. Map each band to a bucket\n",
    "2. Implement the query function that identifies all buckets for a signature and returns LSHcandidates.\n",
    "3. Test with '5kru-rjmf.csv' as the query to identify datasets that have more than 0.90 similarity with the DBN column of '5kru-rjmf.csv'. \n",
    "4. Show the advantage of LSH over the naive quadratic algorithm to calculate most similar pairs of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ab4854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "class LSH:\n",
    "    def __init__(self, num_bands, band_size):\n",
    "        self.num_bands = num_bands\n",
    "        self.band_size = band_size\n",
    "        self.buckets = [{} for _ in range(num_bands)]\n",
    "\n",
    "    def _split_signature(self, minhash_signature):\n",
    "        return [\n",
    "                minhash_signature[i:i+self.band_size]\n",
    "                for i in range(0, len(minhash_signature), self.band_size)\n",
    "               ]\n",
    "\n",
    "    #This function takes a dataset_id and its minhash signature\n",
    "    #The function adds these datasets to respective buckets\n",
    "    def index(self, dataset_id, minhash_signature):\n",
    "        band_keys = self._split_signature(minhash_signature)\n",
    "        for i, band_key in enumerate(band_keys):\n",
    "            band_key = tuple(band_key)\n",
    "            if band_key not in self.buckets[i]:\n",
    "                self.buckets[i][band_key] = []\n",
    "            self.buckets[i][band_key].append(dataset_id)\n",
    "\n",
    "        \n",
    "    #This function takes minhash signature as input and returns a list of candidates\n",
    "    #that share the same bucket as this dataset\n",
    "    def query(self, minhash_signature):\n",
    "        band_keys = self._split_signature(minhash_signature)\n",
    "        candidates = set()\n",
    "        for i, band_key in enumerate(band_keys):\n",
    "            band_key = tuple(band_key)\n",
    "            if band_key in self.buckets[i]:\n",
    "                for candidate in self.buckets[i][band_key]:\n",
    "                    candidates.add(candidate)\n",
    "        return list(candidates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90e6d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You don't have to change num_hashes and num_bands\n",
    "#However, if your results do not show advantage of LSH over the naive algorithm, \n",
    "# you can change these to suit your needs\n",
    "num_hashes = 1000\n",
    "num_bands = 10\n",
    "minhash = MinHash(num_hashes)\n",
    "band_size = num_hashes // num_bands\n",
    "lsh = LSH(num_bands, band_size)\n",
    "files=['5kru-rjmf.csv','23z9-6uk9.csv','29bv-qqsy.csv','5g59-fxev.csv','29ry-u5bf.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce3db2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH Candidates with more than 0.90 similarity:\n",
      "File: 29bv-qqsy.csv, Similarity: 0.9242081447963801\n"
     ]
    }
   ],
   "source": [
    "query_file = '5kru-rjmf.csv'\n",
    "query_df = pd.read_csv(path + query_file)\n",
    "if 'DBN' in query_df.columns:\n",
    "    query_dbn = list(query_df['DBN'].values)\n",
    "else:\n",
    "    query_dbn = list(query_df['dbn'].values)\n",
    "\n",
    "query_signature = minhash.generate_signature(query_dbn)\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    if file == query_file:\n",
    "        continue\n",
    "    df = pd.read_csv(path + file)\n",
    "    if 'DBN' in df.columns:\n",
    "        dbn_list = list(df['DBN'].values)\n",
    "    else:\n",
    "        dbn_list = list(df['dbn'].values)\n",
    "    signature = minhash.generate_signature(dbn_list)\n",
    "    lsh.index(idx, signature)\n",
    "\n",
    "candidates = lsh.query(query_signature)\n",
    "\n",
    "print(\"LSH Candidates with more than 0.90 similarity:\")\n",
    "for candidate in candidates:\n",
    "    candidate_dbn = val_lst[candidate]\n",
    "    similarity = jaccard_original(query_dbn, candidate_dbn)\n",
    "    if similarity > 0.90:\n",
    "        print(f\"File: {files[candidate]}, Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bfb1190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar pair of files using LSH: ('5kru-rjmf.csv', '29bv-qqsy.csv') with similarity 0.9242081447963801\n",
      "Time taken by LSH: 0.0007302761077880859 seconds\n",
      "Most similar pair of files using naive quadratic algorithm: ('5kru-rjmf.csv', '29bv-qqsy.csv') with similarity 0.9242081447963801\n",
      "Time taken by naive quadratic algorithm: 0.0016188621520996094 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "all_dbn_lists = []\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    df = pd.read_csv(path + file)\n",
    "    if 'DBN' in df.columns:\n",
    "        dbn_list = list(df['DBN'].values)\n",
    "    else:\n",
    "        dbn_list = list(df['dbn'].values)\n",
    "    all_dbn_lists.append(dbn_list)\n",
    "\n",
    "all_signature = []\n",
    "for key, dbn_list in enumerate(all_dbn_lists):\n",
    "    signature = minhash.generate_signature(dbn_list)\n",
    "    all_signature.append(signature)\n",
    "    lsh.index(key, signature)\n",
    "\n",
    "# Measure time taken by LSH\n",
    "start_time = time.time()\n",
    "\n",
    "most_similar_score = 0\n",
    "most_similar_pair = None\n",
    "\n",
    "for key, dbn_list in enumerate(all_dbn_lists):\n",
    "    signature = all_signature[key]\n",
    "    candidates = lsh.query(signature)\n",
    "    for candidate in candidates:\n",
    "        if candidate == key:\n",
    "            continue\n",
    "        candidate_dbn_list = all_dbn_lists[candidate]\n",
    "        similarity = jaccard_original(dbn_list, candidate_dbn_list)\n",
    "        if similarity > most_similar_score:\n",
    "            most_similar_score = similarity\n",
    "            most_similar_pair = (files[key], files[candidate])\n",
    "lsh_time = time.time() - start_time\n",
    "\n",
    "print(\"Most similar pair of files using LSH:\", most_similar_pair, \"with similarity\", most_similar_score)\n",
    "print(\"Time taken by LSH:\", lsh_time, \"seconds\")\n",
    "\n",
    "# Measure time taken by naive quadratic algorithm\n",
    "start_time = time.time()\n",
    "most_similar_score = 0\n",
    "most_similar_pair = None\n",
    "for i in range(len(files)):\n",
    "    for j in range(i + 1, len(files)):\n",
    "        similarity = jaccard_original(val_lst[i], val_lst[j])\n",
    "        if similarity > most_similar_score:\n",
    "            most_similar_score = similarity\n",
    "            most_similar_pair = (files[i], files[j])\n",
    "naive_time = time.time() - start_time\n",
    "\n",
    "print(\"Most similar pair of files using naive quadratic algorithm:\", most_similar_pair, \"with similarity\", most_similar_score)\n",
    "print(\"Time taken by naive quadratic algorithm:\", naive_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6386",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
